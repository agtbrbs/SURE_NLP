{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fa4e4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "331ade08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(word):\n",
    "    # split text phrases into words\n",
    "    tokens  = nltk.word_tokenize(word.lower())\n",
    "    \n",
    "    \n",
    "    \n",
    "    custom_puntuations = ['.', ',', '/', '!', '?', ';', ':', '(',')', '[',']', '-', '_', '%', 'et', 'al', \"et al\", \n",
    "                          'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',  'l', 'm', 'n', 'o', 'p', \n",
    "                          'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', \"’\", \"\", \"’\", '”', \"”\",\"‐\", \"WP\", \"wp\", \"Wp\",\n",
    "                          \"|\",\"•\", \".nr\", \"„\", \"”\", \"\", \"ws\", \"e.g\", \"eg\", \"e g\", \"▪\", \"…\", \"§\", \"à\", \"±\", \"’\", \"°c\",\n",
    "                          \"ns\", \"hoc\", \"aalml\", \"aatse\", \n",
    "                         \"–\", \".∙\", \".\", \"‘\", \"→\", \"∙\", \"-\", \"i.e\", \"%\", '“', \"‘\", \"[\", \"]\", \"{\", \"}\", \"•\",\"©\",\"<\", \">\", \"€\"]\n",
    "    # Initialize the stopwords variable, which is a list of words ('and', 'the', 'i', 'yourself', 'is') that do not hold much values as key words\n",
    "    stop_words  = stopwords.words('english')\n",
    "    stop_words.extend(custom_puntuations)\n",
    "    # Getting rid of all the words that contain numbers in them\n",
    "    w_num = re.sub('\\w*\\d\\w*', '', word).strip()\n",
    "\n",
    "    \n",
    "    # Return keywords which are not in stop words \n",
    "    #keyword = word if not word in stop_words and word in w_num and word in punctuations\n",
    "    \n",
    "    #new_words = all(not word in stop_words and word in w_num and word in punctuations for word in words)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for token in tokens:\n",
    "        token = re.sub(r'\\W', ' ', str(token))\n",
    "        token = re.sub('\\w*\\d\\w*', '', token).strip()\n",
    "        # Remove leading/trailing punctuation and convert to lowercase\n",
    "        #token = token.strip(string.punctuation).lower()\n",
    "        # Check if token is not a stop word, contains at least one alphabetic character, and is not a single character\n",
    "        if not token in stop_words:\n",
    "            # Remove all the special characters\n",
    "            if len(token) > 0:\n",
    "                clean_tokens.append(token)\n",
    "    words = \" \".join(clean_tokens)\n",
    "    return words\n",
    "    \n",
    "def POS(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    parts_of_speech = nltk.pos_tag(tokens)\n",
    "    chunking = r\"\"\"\n",
    "      Target: {<NN><NN><NN>}\n",
    "      Target: {<NN><NN>}  \n",
    "      Target: {<JJ>*<NN>}\n",
    "      Target: {<JJ><JJ><JJ>}\n",
    "      Target: {<JJ><JJ>}\n",
    "      Target: {<JJ>}\n",
    "      Verb: {<VB.*>}\n",
    "    \"\"\"\n",
    "    expression = nltk.RegexpParser(chunking)\n",
    "    chunked = expression.parse(parts_of_speech)\n",
    "    \n",
    "    verbs = []\n",
    "    others = []\n",
    "    for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Target'):\n",
    "        word =\" \".join([a for (a,b) in subtree.leaves()])\n",
    "        cleaned = clean_text(word)\n",
    "        if(not any(char.isdigit() for char in word) ):\n",
    "            if len(cleaned) > 0:\n",
    "                others.append(cleaned)\n",
    "\n",
    "    for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Verb'):\n",
    "        word =\" \".join([a for (a,b) in subtree.leaves()])\n",
    "        cleaned = clean_text(word)\n",
    "        if(not any(char.isdigit() for char in word)):\n",
    "            if cleaned is not None:\n",
    "                verbs.append(lemmatizer.lemmatize(cleaned, wordnet.VERB))\n",
    "    \n",
    "    return verbs, others\n",
    "\n",
    "\n",
    "\n",
    "def lines2sentences(lines):\n",
    "    cleaned_lines = []\n",
    "    sentences = []\n",
    "    \n",
    "    for i in lines:\n",
    "        cleaned_lines.append(i)\n",
    "    join_lines = ''.join(cleaned_lines)\n",
    "    sentences = tokenize.sent_tokenize(join_lines)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unique_count(dfs, column):\n",
    "    df = dfs[dfs[column].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "    lists = df[column].explode().to_list()\n",
    "    list_count = [[x,lists.count(x), len(x.split())] for x in set(lists)]\n",
    "    df_words = pd.DataFrame(columns = ['word', 'occurance', 'word_count'])\n",
    "    for i in range(len(list_count)):\n",
    "        df_words.loc[i, \"word\"] = list_count[i][0]\n",
    "        df_words.loc[i, \"occurance\"] = list_count[i][1]\n",
    "        df_words.loc[i, \"word_count\"] = list_count[i][2]\n",
    "    df_words = df_words.sort_values(by=['word_count'], ascending=False)\n",
    "    return df_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "30ce58d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project proposals/LIRLAP.pdf  - start working: \n",
      "Project proposals/LIRLAP.pdf  - total lines:  911\n",
      "Project proposals/LIRLAP.pdf  - total sentences:  451\n",
      "complete verb sheet -  Project proposals/LIRLAP.pdf\n",
      "complete others sheet -  Project proposals/LIRLAP.pdf\n",
      "Project proposals/MYrisk.pdf  - start working: \n",
      "Project proposals/MYrisk.pdf  - total lines:  689\n",
      "Project proposals/MYrisk.pdf  - total sentences:  280\n",
      "complete verb sheet -  Project proposals/MYrisk.pdf\n",
      "complete others sheet -  Project proposals/MYrisk.pdf\n",
      "Project proposals/emplement!.pdf  - start working: \n",
      "Project proposals/emplement!.pdf  - total lines:  975\n",
      "Project proposals/emplement!.pdf  - total sentences:  486\n",
      "complete verb sheet -  Project proposals/emplement!.pdf\n",
      "complete others sheet -  Project proposals/emplement!.pdf\n",
      "Project proposals/GreenCityLabHue.pdf  - start working: \n",
      "Project proposals/GreenCityLabHue.pdf  - total lines:  903\n",
      "Project proposals/GreenCityLabHue.pdf  - total sentences:  428\n",
      "complete verb sheet -  Project proposals/GreenCityLabHue.pdf\n",
      "complete others sheet -  Project proposals/GreenCityLabHue.pdf\n",
      "Project proposals/URA.pdf  - start working: \n",
      "Project proposals/URA.pdf  - total lines:  1035\n",
      "Project proposals/URA.pdf  - total sentences:  312\n",
      "complete verb sheet -  Project proposals/URA.pdf\n",
      "complete others sheet -  Project proposals/URA.pdf\n",
      "Project proposals/Charms.pdf  - start working: \n",
      "Project proposals/Charms.pdf  - total lines:  1106\n",
      "Project proposals/Charms.pdf  - total sentences:  416\n",
      "complete verb sheet -  Project proposals/Charms.pdf\n",
      "complete others sheet -  Project proposals/Charms.pdf\n",
      "Project proposals/BMBF Call.pdf  - start working: \n",
      "Project proposals/BMBF Call.pdf  - total lines:  513\n",
      "Project proposals/BMBF Call.pdf  - total sentences:  193\n",
      "complete verb sheet -  Project proposals/BMBF Call.pdf\n",
      "complete others sheet -  Project proposals/BMBF Call.pdf\n",
      "Project proposals/SURE.pdf  - start working: \n",
      "Project proposals/SURE.pdf  - total lines:  357\n",
      "Project proposals/SURE.pdf  - total sentences:  154\n",
      "complete verb sheet -  Project proposals/SURE.pdf\n",
      "complete others sheet -  Project proposals/SURE.pdf\n",
      "Project proposals/IMECOGIP.pdf  - start working: \n",
      "Project proposals/IMECOGIP.pdf  - total lines:  691\n",
      "Project proposals/IMECOGIP.pdf  - total sentences:  361\n",
      "complete verb sheet -  Project proposals/IMECOGIP.pdf\n",
      "complete others sheet -  Project proposals/IMECOGIP.pdf\n",
      "Project proposals/FloodAdaptVN.pdf  - start working: \n",
      "Project proposals/FloodAdaptVN.pdf  - total lines:  1258\n",
      "Project proposals/FloodAdaptVN.pdf  - total sentences:  535\n",
      "complete verb sheet -  Project proposals/FloodAdaptVN.pdf\n",
      "complete others sheet -  Project proposals/FloodAdaptVN.pdf\n",
      "Project proposals/Polyurbanwaters.pdf  - start working: \n",
      "Project proposals/Polyurbanwaters.pdf  - total lines:  807\n",
      "Project proposals/Polyurbanwaters.pdf  - total sentences:  359\n",
      "complete verb sheet -  Project proposals/Polyurbanwaters.pdf\n",
      "complete others sheet -  Project proposals/Polyurbanwaters.pdf\n",
      "Project proposals/Build4People.pdf  - start working: \n",
      "Project proposals/Build4People.pdf  - total lines:  7865\n",
      "Project proposals/Build4People.pdf  - total sentences:  2038\n",
      "complete verb sheet -  Project proposals/Build4People.pdf\n",
      "complete others sheet -  Project proposals/Build4People.pdf\n",
      "Policies/IPCC-6th-edition-outline.pdf  - start working: \n",
      "Policies/IPCC-6th-edition-outline.pdf  - total lines:  560\n",
      "Policies/IPCC-6th-edition-outline.pdf  - total sentences:  244\n",
      "complete verb sheet -  Policies/IPCC-6th-edition-outline.pdf\n",
      "complete others sheet -  Policies/IPCC-6th-edition-outline.pdf\n",
      "Policies/paris-agreement.pdf  - start working: \n",
      "Policies/paris-agreement.pdf  - total lines:  1544\n",
      "Policies/paris-agreement.pdf  - total sentences:  426\n",
      "complete verb sheet -  Policies/paris-agreement.pdf\n",
      "complete others sheet -  Policies/paris-agreement.pdf\n",
      "Policies/bmbf_fona31.pdf  - start working: \n",
      "Policies/bmbf_fona31.pdf  - total lines:  2029\n",
      "Policies/bmbf_fona31.pdf  - total sentences:  651\n",
      "complete verb sheet -  Policies/bmbf_fona31.pdf\n",
      "complete others sheet -  Policies/bmbf_fona31.pdf\n",
      "Policies/New-urban-agenda.pdf  - start working: \n",
      "Policies/New-urban-agenda.pdf  - total lines:  1280\n",
      "Policies/New-urban-agenda.pdf  - total sentences:  337\n",
      "complete verb sheet -  Policies/New-urban-agenda.pdf\n",
      "complete others sheet -  Policies/New-urban-agenda.pdf\n",
      "Policies/IPCC-5th-edition-full.pdf  - start working: \n",
      "Policies/IPCC-5th-edition-full.pdf  - total lines:  3116\n",
      "Policies/IPCC-5th-edition-full.pdf  - total sentences:  1554\n",
      "complete verb sheet -  Policies/IPCC-5th-edition-full.pdf\n",
      "complete others sheet -  Policies/IPCC-5th-edition-full.pdf\n",
      "Policies/WBGU.pdf  - start working: \n",
      "Policies/WBGU.pdf  - total lines:  25019\n",
      "Policies/WBGU.pdf  - total sentences:  11286\n",
      "complete verb sheet -  Policies/WBGU.pdf\n",
      "complete others sheet -  Policies/WBGU.pdf\n",
      "Policies/FONA_4.pdf  - start working: \n",
      "Policies/FONA_4.pdf  - total lines:  2084\n",
      "Policies/FONA_4.pdf  - total sentences:  804\n",
      "complete verb sheet -  Policies/FONA_4.pdf\n",
      "complete others sheet -  Policies/FONA_4.pdf\n"
     ]
    }
   ],
   "source": [
    "directories = ['Project proposals', 'Policies']\n",
    "paths = []\n",
    "for directory in directories:\n",
    "    files = os.listdir(directory)\n",
    "    for file in files:\n",
    "        path = os.path.join(directory, file)\n",
    "        paths.append(path)\n",
    "\n",
    "for file in paths:\n",
    "    baseName = os.path.basename(file)\n",
    "    fileName = os.path.splitext(baseName)[0]\n",
    "    extention = os.path.splitext(baseName)[1]\n",
    "    if extention == '.pdf':\n",
    "        print(file, \" - start working: \")\n",
    "        pdf = pdfplumber.open(file)\n",
    "        \n",
    "        pages_content = []\n",
    "        lines = []\n",
    "        for page in pdf.pages:\n",
    "            pages_content += page.extract_text().split(\"\\n\")\n",
    "\n",
    "\n",
    "        for i in pages_content:\n",
    "            if i[-1] == \"-\":\n",
    "                i = i[:-1]\n",
    "            lines.append(i)\n",
    "        df = pd.DataFrame(columns = ['sentence', 'others', 'verbs'])\n",
    "        print(file, \" - total lines: \", len(lines))\n",
    "        \n",
    "        sentences = lines2sentences(lines)\n",
    "        print(file, \" - total sentences: \", len(sentences))\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            pos = POS(sentence)\n",
    "            df.loc[i, \"sentence\"] = sentence\n",
    "            df.loc[i, \"verbs\"] = pos[0]\n",
    "            df.loc[i, \"others\"] = pos[1]\n",
    "        \n",
    "        verbs = unique_count(df,'verbs')\n",
    "        others = unique_count(df,'others')\n",
    "        \n",
    "        with pd.ExcelWriter('verbs.xlsx', engine ='openpyxl', mode = 'a', if_sheet_exists = \"replace\") as writer:\n",
    "            verbs.to_excel(writer, sheet_name = fileName,index=False)\n",
    "            writer.save()\n",
    "        print(\"complete verb sheet - \", file)\n",
    "        \n",
    "        with pd.ExcelWriter('others.xlsx', engine ='openpyxl', mode = 'a', if_sheet_exists = \"replace\") as writer:\n",
    "            others.to_excel(writer, sheet_name = fileName,index=False)\n",
    "            writer.save()\n",
    "        print(\"complete others sheet - \", file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a4ca6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Issues\n",
    "# - extend puntuations from previous list\n",
    "# \"–\", \".∙\", \".\", \"‘\", \"→\", \"∙\", \"-\", \"i.e\", \"%\", '“', \"‘\", \"[\", \"]\", \"{\", \"}\", \"•\",\"©\",\"<\", \">\", \"€\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f307e789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39180, 3)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffull.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b757d61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = dffull.groupby('word').agg({'occurance': 'sum', 'word_count': 'first'})\n",
    "df_unique = df_unique.sort_values(by=['occurance'], ascending=False)\n",
    "df_unique = df_unique.reset_index()\n",
    "with pd.ExcelWriter('full.xlsx', engine ='openpyxl', mode = 'a', if_sheet_exists = \"replace\") as writer:\n",
    "    df_unique.to_excel(writer,index=False)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cc8f58fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>occurance</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urban</td>\n",
       "      <td>1150</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>city</td>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>local</td>\n",
       "      <td>869</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development</td>\n",
       "      <td>799</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>research</td>\n",
       "      <td>760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27664</th>\n",
       "      <td>great strain</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27665</th>\n",
       "      <td>great spatial</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27666</th>\n",
       "      <td>great social heterogeneity</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27667</th>\n",
       "      <td>great social diversity</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27668</th>\n",
       "      <td>ﬁnancial</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27669 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             word occurance word_count\n",
       "0                           urban      1150          1\n",
       "1                            city      1004          1\n",
       "2                           local       869          1\n",
       "3                     development       799          1\n",
       "4                        research       760          1\n",
       "...                           ...       ...        ...\n",
       "27664                great strain         1          2\n",
       "27665               great spatial         1          2\n",
       "27666  great social heterogeneity         1          3\n",
       "27667      great social diversity         1          3\n",
       "27668                    ﬁnancial         1          1\n",
       "\n",
       "[27669 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ac20a34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " '-is',\n",
       " '#',\n",
       " 'great',\n",
       " \"...that's\",\n",
       " 'project',\n",
       " '9',\n",
       " 'sho9uld',\n",
       " 'be',\n",
       " '’projects10']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = \"this -is # great ...that's project 9 sho9uld be ’projects10\"\n",
    "r = p.split()\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c0a7d939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " '#',\n",
       " 'great',\n",
       " 'that',\n",
       " \"'s\",\n",
       " 'project',\n",
       " 'should',\n",
       " 'be',\n",
       " '’',\n",
       " 'projects']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "601626c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# great 's project projects\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "669630f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this  is   great    that s project 9 sho9uld be  projects10'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r'\\W', ' ', str(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "369e2a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this is # great that's project   be ’\""
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub('\\w*\\d\\w*', '', p).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "83206a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tp/vtc9vg1s2vzcs0f1bvndrf1r0000gn/T/ipykernel_47073/1615974658.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m re.sub('\\w*\\d\\w*', '', ['this',\n\u001b[0m\u001b[1;32m      2\u001b[0m  \u001b[0;34m'-is'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m  \u001b[0;34m'#'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m  \u001b[0;34m'great'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m  \u001b[0;34m\"...that's\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "re.sub('\\w*\\d\\w*', '', ['this',\n",
    " '-is',\n",
    " '#',\n",
    " 'great',\n",
    " \"...that's\",\n",
    " 'project',\n",
    " '9',\n",
    " 'sho9uld',\n",
    " 'be',\n",
    " '’projects10']).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "29a9ca17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6ffbb8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "de39c74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "if len(p) > 0:\n",
    "    print(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28948b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
