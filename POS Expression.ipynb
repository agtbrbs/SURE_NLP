{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa4e4776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pdfplumber\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk import tokenize\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "331ade08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(word):\n",
    "    # split text phrases into words\n",
    "    words  = nltk.word_tokenize(word)\n",
    "    \n",
    "    # Remove all the special characters\n",
    "    punctuations = re.sub(r'\\W', ' ', str(word))\n",
    "    \n",
    "    custom_puntuations = ['.', ',', '/', '!', '?', ';', ':', '(',')', '[',']', '-', '_', '%', 'et', 'al', \"et al\", 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k',  'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    # Initialize the stopwords variable, which is a list of words ('and', 'the', 'i', 'yourself', 'is') that do not hold much values as key words\n",
    "    stop_words  = stopwords.words('english')\n",
    "    stop_words.extend(custom_puntuations)\n",
    "    # Getting rid of all the words that contain numbers in them\n",
    "    w_num = re.sub('\\w*\\d\\w*', '', word).strip()\n",
    "\n",
    "    \n",
    "    # Return keywords which are not in stop words \n",
    "    #keyword = word if not word in stop_words and word in w_num and word in punctuations\n",
    "    \n",
    "    if not word in stop_words and word in w_num and word in punctuations:\n",
    "        return word\n",
    "    \n",
    "def POS(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    parts_of_speech = nltk.pos_tag(tokens)\n",
    "    chunking = r\"\"\"\n",
    "        NP: {<JJ>*<NN.*>}          # Noun phrase\n",
    "            {<NN.*><IN><DT>?<JJ>*<NN.*>}  # Noun phrase with preposition\n",
    "            {<DT><JJ>*<NN.*>}          # Determiner + Noun phrase\n",
    "            {<VBG><IN><DT>?<JJ>*<NN.*>}  # Gerund phrase\n",
    "            {<TO><VB><DT>?<JJ>*<NN.*>}   # Infinitive phrase\n",
    "            {<VB.*><IN>?<DT>?<JJ>*<NN.*>}\n",
    "            {<IN><DT>?<JJ>*<NN.*>}\n",
    "    \"\"\"\n",
    "    expression = nltk.RegexpParser(chunking)\n",
    "    chunked = expression.parse(parts_of_speech)\n",
    "    \n",
    "    others = []\n",
    "    for subtree in chunked.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        word =\" \".join([a for (a,b) in subtree.leaves()])\n",
    "        if(not any(char.isdigit() for char in word)):\n",
    "            others.append(word)\n",
    "    \n",
    "    return others\n",
    "\n",
    "\n",
    "\n",
    "def lines2sentences(lines):\n",
    "    cleaned_lines = []\n",
    "    sentences = []\n",
    "    \n",
    "    for i in lines:\n",
    "        cleaned_lines.append(i)\n",
    "    join_lines = ''.join(cleaned_lines)\n",
    "    sentences = tokenize.sent_tokenize(join_lines)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unique_count(dfs, column):\n",
    "    df = dfs[dfs[column].apply(lambda x: isinstance(x, list) and len(x) > 0)]\n",
    "\n",
    "    lists = df[column].explode().to_list()\n",
    "    list_count = [[x,lists.count(x), len(x.split())] for x in set(lists)]\n",
    "    df_words = pd.DataFrame(columns = ['word', 'occurance', 'word_count'])\n",
    "    for i in range(len(list_count)):\n",
    "        df_words.loc[i, \"word\"] = list_count[i][0]\n",
    "        df_words.loc[i, \"occurance\"] = list_count[i][1]\n",
    "        df_words.loc[i, \"word_count\"] = list_count[i][2]\n",
    "    df_words = df_words.sort_values(by=['word_count'], ascending=False)\n",
    "    return df_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30ce58d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project proposals/LIRLAP.pdf  - start working: \n",
      "Project proposals/LIRLAP.pdf  - total lines:  911\n",
      "Project proposals/LIRLAP.pdf  - total sentences:  451\n",
      "complete others sheet -  Project proposals/LIRLAP.pdf\n",
      "Project proposals/MYrisk.pdf  - start working: \n",
      "Project proposals/MYrisk.pdf  - total lines:  689\n",
      "Project proposals/MYrisk.pdf  - total sentences:  280\n",
      "complete others sheet -  Project proposals/MYrisk.pdf\n",
      "Project proposals/emplement!.pdf  - start working: \n",
      "Project proposals/emplement!.pdf  - total lines:  975\n",
      "Project proposals/emplement!.pdf  - total sentences:  486\n",
      "complete others sheet -  Project proposals/emplement!.pdf\n",
      "Project proposals/GreenCityLabHue.pdf  - start working: \n",
      "Project proposals/GreenCityLabHue.pdf  - total lines:  903\n",
      "Project proposals/GreenCityLabHue.pdf  - total sentences:  428\n",
      "complete others sheet -  Project proposals/GreenCityLabHue.pdf\n",
      "Project proposals/URA.pdf  - start working: \n",
      "Project proposals/URA.pdf  - total lines:  1035\n",
      "Project proposals/URA.pdf  - total sentences:  312\n",
      "complete others sheet -  Project proposals/URA.pdf\n",
      "Project proposals/Charms.pdf  - start working: \n",
      "Project proposals/Charms.pdf  - total lines:  1106\n",
      "Project proposals/Charms.pdf  - total sentences:  416\n",
      "complete others sheet -  Project proposals/Charms.pdf\n",
      "Project proposals/SURE.pdf  - start working: \n",
      "Project proposals/SURE.pdf  - total lines:  357\n",
      "Project proposals/SURE.pdf  - total sentences:  154\n",
      "complete others sheet -  Project proposals/SURE.pdf\n",
      "Project proposals/IMECOGIP.pdf  - start working: \n",
      "Project proposals/IMECOGIP.pdf  - total lines:  691\n",
      "Project proposals/IMECOGIP.pdf  - total sentences:  361\n",
      "complete others sheet -  Project proposals/IMECOGIP.pdf\n",
      "Project proposals/FloodAdaptVN.pdf  - start working: \n",
      "Project proposals/FloodAdaptVN.pdf  - total lines:  1258\n",
      "Project proposals/FloodAdaptVN.pdf  - total sentences:  535\n",
      "complete others sheet -  Project proposals/FloodAdaptVN.pdf\n",
      "Project proposals/Polyurbanwaters.pdf  - start working: \n",
      "Project proposals/Polyurbanwaters.pdf  - total lines:  807\n",
      "Project proposals/Polyurbanwaters.pdf  - total sentences:  359\n",
      "complete others sheet -  Project proposals/Polyurbanwaters.pdf\n",
      "Project proposals/Build4People.pdf  - start working: \n",
      "Project proposals/Build4People.pdf  - total lines:  7865\n",
      "Project proposals/Build4People.pdf  - total sentences:  2038\n",
      "complete others sheet -  Project proposals/Build4People.pdf\n",
      "Policies/IPCC-6th-edition-outline.pdf  - start working: \n",
      "Policies/IPCC-6th-edition-outline.pdf  - total lines:  560\n",
      "Policies/IPCC-6th-edition-outline.pdf  - total sentences:  244\n",
      "complete others sheet -  Policies/IPCC-6th-edition-outline.pdf\n",
      "Policies/paris-agreement.pdf  - start working: \n",
      "Policies/paris-agreement.pdf  - total lines:  1544\n",
      "Policies/paris-agreement.pdf  - total sentences:  426\n",
      "complete others sheet -  Policies/paris-agreement.pdf\n",
      "Policies/New-urban-agenda.pdf  - start working: \n",
      "Policies/New-urban-agenda.pdf  - total lines:  1280\n",
      "Policies/New-urban-agenda.pdf  - total sentences:  337\n",
      "complete others sheet -  Policies/New-urban-agenda.pdf\n",
      "Policies/IPCC-5th-edition-full.pdf  - start working: \n",
      "Policies/IPCC-5th-edition-full.pdf  - total lines:  3116\n",
      "Policies/IPCC-5th-edition-full.pdf  - total sentences:  1554\n",
      "complete others sheet -  Policies/IPCC-5th-edition-full.pdf\n",
      "Policies/WBGU.pdf  - start working: \n",
      "Policies/WBGU.pdf  - total lines:  25019\n",
      "Policies/WBGU.pdf  - total sentences:  11286\n",
      "complete others sheet -  Policies/WBGU.pdf\n",
      "Policies/FONA.pdf  - start working: \n",
      "Policies/FONA.pdf  - total lines:  2084\n",
      "Policies/FONA.pdf  - total sentences:  804\n",
      "complete others sheet -  Policies/FONA.pdf\n"
     ]
    }
   ],
   "source": [
    "directories = ['Project proposals', 'Policies']\n",
    "paths = []\n",
    "for directory in directories:\n",
    "    files = os.listdir(directory)\n",
    "    for file in files:\n",
    "        path = os.path.join(directory, file)\n",
    "        paths.append(path)\n",
    "\n",
    "for file in paths:\n",
    "    baseName = os.path.basename(file)\n",
    "    fileName = os.path.splitext(baseName)[0]\n",
    "    extention = os.path.splitext(baseName)[1]\n",
    "    if extention == '.pdf':\n",
    "        print(file, \" - start working: \")\n",
    "        pdf = pdfplumber.open(file)\n",
    "        \n",
    "        pages_content = []\n",
    "        lines = []\n",
    "        for page in pdf.pages:\n",
    "            pages_content += page.extract_text().split(\"\\n\")\n",
    "\n",
    "\n",
    "        for i in pages_content:\n",
    "            if i[-1] == \"-\":\n",
    "                i = i[:-1]\n",
    "            lines.append(i)\n",
    "        df = pd.DataFrame(columns = ['sentence', 'others'])\n",
    "        print(file, \" - total lines: \", len(lines))\n",
    "        \n",
    "        sentences = lines2sentences(lines)\n",
    "        print(file, \" - total sentences: \", len(sentences))\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = sentences[i]\n",
    "            pos = POS(sentence)\n",
    "            df.loc[i, \"sentence\"] = sentence\n",
    "            df.loc[i, \"others\"] = pos\n",
    "        \n",
    "        others = unique_count(df,'others')\n",
    "        \n",
    "        with pd.ExcelWriter('expressions.xlsx', engine ='openpyxl', mode = 'a', if_sheet_exists = \"replace\") as writer:\n",
    "            others.to_excel(writer, sheet_name = fileName,index=False)\n",
    "            writer.save()\n",
    "        print(\"complete others sheet - \", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a4ca6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Issues\n",
    "# - extend puntuations from previous list\n",
    "# \"–\", \".∙\", \".\", \"‘\", \"→\", \"∙\", \"-\", \"i.e\", \"%\", '“', \"‘\", \"[\", \"]\", \"{\", \"}\", \"•\"\n",
    "# \"©\",\"<\", \">\", \"€\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f307e789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
